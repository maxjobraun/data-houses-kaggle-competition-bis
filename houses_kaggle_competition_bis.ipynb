{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Houses Kaggle Competition (revisited with Deep Learning üî•) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/ML/kaggle-batch-challenge.png' width=600>](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "‚öôÔ∏è Let's re-use our previous **pipeline** built in the module **`05-07-Ensemble-Methods`** and try to improve our final predictions with a Neural Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# DATA MANIPULATION\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "# DATA VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# VIEWING OPTIONS IN THE NOTEBOOK\n",
    "from sklearn import set_config; set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) üöÄ Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Load the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíæ Let's load our **training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_train_raw.csv\")\n",
    "X = data.drop(columns='SalePrice')\n",
    "y = data['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofMatl</th>\n",
       "      <th>Exterior1st</th>\n",
       "      <th>Exterior2nd</th>\n",
       "      <th>MasVnrType</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "      <th>Foundation</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>Heating</th>\n",
       "      <th>HeatingQC</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>Electrical</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Functional</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>706</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>8</td>\n",
       "      <td>Typ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>548</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>Feedr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>978</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>162.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Mn</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>486</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>608</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Wd Sdng</td>\n",
       "      <td>Wd Shng</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>BrkTil</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>No</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>216</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>7</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Detchd</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>3</td>\n",
       "      <td>642</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Av</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>655</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>9</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>3</td>\n",
       "      <td>836</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities LotConfig LandSlope Neighborhood Condition1  \\\n",
       "0         Lvl    AllPub    Inside       Gtl      CollgCr       Norm   \n",
       "1         Lvl    AllPub       FR2       Gtl      Veenker      Feedr   \n",
       "2         Lvl    AllPub    Inside       Gtl      CollgCr       Norm   \n",
       "3         Lvl    AllPub    Corner       Gtl      Crawfor       Norm   \n",
       "4         Lvl    AllPub       FR2       Gtl      NoRidge       Norm   \n",
       "\n",
       "  Condition2 BldgType HouseStyle  OverallQual  OverallCond  YearBuilt  \\\n",
       "0       Norm     1Fam     2Story            7            5       2003   \n",
       "1       Norm     1Fam     1Story            6            8       1976   \n",
       "2       Norm     1Fam     2Story            7            5       2001   \n",
       "3       Norm     1Fam     2Story            7            5       1915   \n",
       "4       Norm     1Fam     2Story            8            5       2000   \n",
       "\n",
       "   YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType  \\\n",
       "0          2003     Gable  CompShg     VinylSd     VinylSd    BrkFace   \n",
       "1          1976     Gable  CompShg     MetalSd     MetalSd       None   \n",
       "2          2002     Gable  CompShg     VinylSd     VinylSd    BrkFace   \n",
       "3          1970     Gable  CompShg     Wd Sdng     Wd Shng       None   \n",
       "4          2000     Gable  CompShg     VinylSd     VinylSd    BrkFace   \n",
       "\n",
       "   MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure  \\\n",
       "0       196.0        Gd        TA      PConc       Gd       TA           No   \n",
       "1         0.0        TA        TA     CBlock       Gd       TA           Gd   \n",
       "2       162.0        Gd        TA      PConc       Gd       TA           Mn   \n",
       "3         0.0        TA        TA     BrkTil       TA       Gd           No   \n",
       "4       350.0        Gd        TA      PConc       Gd       TA           Av   \n",
       "\n",
       "  BsmtFinType1  BsmtFinSF1 BsmtFinType2  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  \\\n",
       "0          GLQ         706          Unf           0        150          856   \n",
       "1          ALQ         978          Unf           0        284         1262   \n",
       "2          GLQ         486          Unf           0        434          920   \n",
       "3          ALQ         216          Unf           0        540          756   \n",
       "4          GLQ         655          Unf           0        490         1145   \n",
       "\n",
       "  Heating HeatingQC CentralAir Electrical  1stFlrSF  2ndFlrSF  LowQualFinSF  \\\n",
       "0    GasA        Ex          Y      SBrkr       856       854             0   \n",
       "1    GasA        Ex          Y      SBrkr      1262         0             0   \n",
       "2    GasA        Ex          Y      SBrkr       920       866             0   \n",
       "3    GasA        Gd          Y      SBrkr       961       756             0   \n",
       "4    GasA        Ex          Y      SBrkr      1145      1053             0   \n",
       "\n",
       "   GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  \\\n",
       "0       1710             1             0         2         1             3   \n",
       "1       1262             0             1         2         0             3   \n",
       "2       1786             1             0         2         1             3   \n",
       "3       1717             1             0         1         0             3   \n",
       "4       2198             1             0         2         1             4   \n",
       "\n",
       "   KitchenAbvGr KitchenQual  TotRmsAbvGrd Functional  Fireplaces FireplaceQu  \\\n",
       "0             1          Gd             8        Typ           0         NaN   \n",
       "1             1          TA             6        Typ           1          TA   \n",
       "2             1          Gd             6        Typ           1          TA   \n",
       "3             1          Gd             7        Typ           1          Gd   \n",
       "4             1          Gd             9        Typ           1          TA   \n",
       "\n",
       "  GarageType  GarageYrBlt GarageFinish  GarageCars  GarageArea GarageQual  \\\n",
       "0     Attchd       2003.0          RFn           2         548         TA   \n",
       "1     Attchd       1976.0          RFn           2         460         TA   \n",
       "2     Attchd       2001.0          RFn           2         608         TA   \n",
       "3     Detchd       1998.0          Unf           3         642         TA   \n",
       "4     Attchd       2000.0          RFn           3         836         TA   \n",
       "\n",
       "  GarageCond PavedDrive  WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  \\\n",
       "0         TA          Y           0           61              0          0   \n",
       "1         TA          Y         298            0              0          0   \n",
       "2         TA          Y           0           42              0          0   \n",
       "3         TA          Y           0           35            272          0   \n",
       "4         TA          Y         192           84              0          0   \n",
       "\n",
       "   ScreenPorch  PoolArea PoolQC Fence MiscFeature  MiscVal  MoSold  YrSold  \\\n",
       "0            0         0    NaN   NaN         NaN        0       2    2008   \n",
       "1            0         0    NaN   NaN         NaN        0       5    2007   \n",
       "2            0         0    NaN   NaN         NaN        0       9    2008   \n",
       "3            0         0    NaN   NaN         NaN        0       2    2006   \n",
       "4            0         0    NaN   NaN         NaN        0      12    2008   \n",
       "\n",
       "  SaleType SaleCondition  \n",
       "0       WD        Normal  \n",
       "1       WD        Normal  \n",
       "2       WD        Normal  \n",
       "3       WD       Abnorml  \n",
       "4       WD        Normal  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 80), (1460,))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíæ Let's also load the **test set**\n",
    "\n",
    "‚ùóÔ∏è Remember ‚ùóÔ∏è You have access to `X_test` but only Kaggle has `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_test_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 80)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Train/Val Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Holdout** ‚ùì \n",
    "\n",
    "As you are not allowed to use the test set (and you don't have access to `y_test` anyway), split your dataset into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Import the preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ You will find in `utils/preprocessor.py` the **`data-preprocessing pipeline`** that was built in our previous iteration.\n",
    "\n",
    "‚ùì Run the cell below, and make sure you understand what the pipeline does. Look at the code in `preprocessor.py` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;numerical_encoder&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;knnimputer&#x27;,\n",
       "                                                                   KNNImputer()),\n",
       "                                                                  (&#x27;minmaxscaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;3SsnPorch&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;BsmtUnfSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;,\n",
       "                                                   &#x27;Fireplaces&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;GarageCars...\n",
       "                                                   &#x27;CentralAir&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;Exterior2nd&#x27;, &#x27;Foundation&#x27;,\n",
       "                                                   &#x27;GarageType&#x27;, &#x27;Heating&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                                   &#x27;MSZoning&#x27;, &#x27;MasVnrType&#x27;,\n",
       "                                                   &#x27;MiscFeature&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;RoofMatl&#x27;,\n",
       "                                                   &#x27;RoofStyle&#x27;, &#x27;SaleCondition&#x27;,\n",
       "                                                   &#x27;SaleType&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Utilities&#x27;])])),\n",
       "                (&#x27;selectpercentile&#x27;,\n",
       "                 SelectPercentile(percentile=75,\n",
       "                                  score_func=&lt;function mutual_info_regression at 0x14758b490&gt;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\" ><label for=\"sk-estimator-id-66\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;numerical_encoder&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;knnimputer&#x27;,\n",
       "                                                                   KNNImputer()),\n",
       "                                                                  (&#x27;minmaxscaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;3SsnPorch&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;BsmtUnfSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;,\n",
       "                                                   &#x27;Fireplaces&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;GarageCars...\n",
       "                                                   &#x27;CentralAir&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;Exterior2nd&#x27;, &#x27;Foundation&#x27;,\n",
       "                                                   &#x27;GarageType&#x27;, &#x27;Heating&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                                   &#x27;MSZoning&#x27;, &#x27;MasVnrType&#x27;,\n",
       "                                                   &#x27;MiscFeature&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;RoofMatl&#x27;,\n",
       "                                                   &#x27;RoofStyle&#x27;, &#x27;SaleCondition&#x27;,\n",
       "                                                   &#x27;SaleType&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Utilities&#x27;])])),\n",
       "                (&#x27;selectpercentile&#x27;,\n",
       "                 SelectPercentile(percentile=75,\n",
       "                                  score_func=&lt;function mutual_info_regression at 0x14758b490&gt;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-67\" type=\"checkbox\" ><label for=\"sk-estimator-id-67\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;numerical_encoder&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;knnimputer&#x27;, KNNImputer()),\n",
       "                                                 (&#x27;minmaxscaler&#x27;,\n",
       "                                                  MinMaxScaler())]),\n",
       "                                 [&#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;3SsnPorch&#x27;,\n",
       "                                  &#x27;BedroomAbvGr&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;,\n",
       "                                  &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;BsmtUnfSF&#x27;,\n",
       "                                  &#x27;EnclosedPorch&#x27;, &#x27;Fireplaces&#x27;, &#x27;FullBath&#x27;,\n",
       "                                  &#x27;GarageArea&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageYrBlt&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;HalfBath...\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;onehotencoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;Alley&#x27;, &#x27;BldgType&#x27;, &#x27;CentralAir&#x27;,\n",
       "                                  &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;Foundation&#x27;, &#x27;GarageType&#x27;,\n",
       "                                  &#x27;Heating&#x27;, &#x27;HouseStyle&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;MSZoning&#x27;, &#x27;MasVnrType&#x27;, &#x27;MiscFeature&#x27;,\n",
       "                                  &#x27;Neighborhood&#x27;, &#x27;RoofMatl&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                  &#x27;SaleCondition&#x27;, &#x27;SaleType&#x27;, &#x27;Street&#x27;,\n",
       "                                  &#x27;Utilities&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\" ><label for=\"sk-estimator-id-68\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;3SsnPorch&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;Fireplaces&#x27;, &#x27;FullBath&#x27;, &#x27;GarageArea&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GrLivArea&#x27;, &#x27;HalfBath&#x27;, &#x27;Id&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;LotArea&#x27;, &#x27;LotFrontage&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;MSSubClass&#x27;, &#x27;MasVnrArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;OverallCond&#x27;, &#x27;OverallQual&#x27;, &#x27;PoolArea&#x27;, &#x27;ScreenPorch&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;YrSold&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-69\" type=\"checkbox\" ><label for=\"sk-estimator-id-69\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNNImputer</label><div class=\"sk-toggleable__content\"><pre>KNNImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-70\" type=\"checkbox\" ><label for=\"sk-estimator-id-70\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\" ><label for=\"sk-estimator-id-71\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ordinal_encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;BsmtQual&#x27;, &#x27;Electrical&#x27;, &#x27;ExterCond&#x27;, &#x27;ExterQual&#x27;, &#x27;Fence&#x27;, &#x27;FireplaceQu&#x27;, &#x27;Functional&#x27;, &#x27;GarageCond&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;HeatingQC&#x27;, &#x27;KitchenQual&#x27;, &#x27;LandContour&#x27;, &#x27;LandSlope&#x27;, &#x27;LotShape&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-72\" type=\"checkbox\" ><label for=\"sk-estimator-id-72\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=&#x27;missing&#x27;, strategy=&#x27;constant&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OrdinalEncoder</label><div class=\"sk-toggleable__content\"><pre>OrdinalEncoder(categories=[[&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;No&#x27;, &#x27;Mn&#x27;, &#x27;Av&#x27;, &#x27;Gd&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Unf&#x27;, &#x27;LwQ&#x27;, &#x27;Rec&#x27;, &#x27;BLQ&#x27;, &#x27;ALQ&#x27;,\n",
       "                            &#x27;GLQ&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Unf&#x27;, &#x27;LwQ&#x27;, &#x27;Rec&#x27;, &#x27;BLQ&#x27;, &#x27;ALQ&#x27;,\n",
       "                            &#x27;GLQ&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Mix&#x27;, &#x27;FuseP&#x27;, &#x27;FuseF&#x27;, &#x27;FuseA&#x27;,\n",
       "                            &#x27;SBrkr&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;...\n",
       "                           [&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Unf&#x27;, &#x27;RFn&#x27;, &#x27;Fin&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Po&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Fa&#x27;, &#x27;TA&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Low&#x27;, &#x27;Bnk&#x27;, &#x27;HLS&#x27;, &#x27;Lvl&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Sev&#x27;, &#x27;Mod&#x27;, &#x27;Gtl&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;IR3&#x27;, &#x27;IR2&#x27;, &#x27;IR1&#x27;, &#x27;Reg&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;N&#x27;, &#x27;P&#x27;, &#x27;Y&#x27;],\n",
       "                           [&#x27;missing&#x27;, &#x27;Fa&#x27;, &#x27;Gd&#x27;, &#x27;Ex&#x27;]],\n",
       "               handle_unknown=&#x27;use_encoded_value&#x27;, unknown_value=-1)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">nominal_encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Alley&#x27;, &#x27;BldgType&#x27;, &#x27;CentralAir&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;Foundation&#x27;, &#x27;GarageType&#x27;, &#x27;Heating&#x27;, &#x27;HouseStyle&#x27;, &#x27;LotConfig&#x27;, &#x27;MSZoning&#x27;, &#x27;MasVnrType&#x27;, &#x27;MiscFeature&#x27;, &#x27;Neighborhood&#x27;, &#x27;RoofMatl&#x27;, &#x27;RoofStyle&#x27;, &#x27;SaleCondition&#x27;, &#x27;SaleType&#x27;, &#x27;Street&#x27;, &#x27;Utilities&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\" ><label for=\"sk-estimator-id-77\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\" ><label for=\"sk-estimator-id-78\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectPercentile</label><div class=\"sk-toggleable__content\"><pre>SelectPercentile(percentile=75,\n",
       "                 score_func=&lt;function mutual_info_regression at 0x14758b490&gt;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('numerical_encoder',\n",
       "                                                  Pipeline(steps=[('knnimputer',\n",
       "                                                                   KNNImputer()),\n",
       "                                                                  ('minmaxscaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['1stFlrSF', '2ndFlrSF',\n",
       "                                                   '3SsnPorch', 'BedroomAbvGr',\n",
       "                                                   'BsmtFinSF1', 'BsmtFinSF2',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'BsmtUnfSF',\n",
       "                                                   'EnclosedPorch',\n",
       "                                                   'Fireplaces', 'FullBath',\n",
       "                                                   'GarageArea', 'GarageCars...\n",
       "                                                   'CentralAir', 'Condition1',\n",
       "                                                   'Condition2', 'Exterior1st',\n",
       "                                                   'Exterior2nd', 'Foundation',\n",
       "                                                   'GarageType', 'Heating',\n",
       "                                                   'HouseStyle', 'LotConfig',\n",
       "                                                   'MSZoning', 'MasVnrType',\n",
       "                                                   'MiscFeature',\n",
       "                                                   'Neighborhood', 'RoofMatl',\n",
       "                                                   'RoofStyle', 'SaleCondition',\n",
       "                                                   'SaleType', 'Street',\n",
       "                                                   'Utilities'])])),\n",
       "                ('selectpercentile',\n",
       "                 SelectPercentile(percentile=75,\n",
       "                                  score_func=<function mutual_info_regression at 0x14758b490>))])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.preprocessor import create_preproc\n",
    "\n",
    "preproc = create_preproc(X_train)\n",
    "preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Scaling your numerical features and encoding the categorical features** ‚ùì\n",
    "\n",
    "Apply these transformations to _both_ your training set and your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": [
     "challengify"
    ],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 159), (438, 159), (1459, 159))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.fit(X_train, y_train)\n",
    "X_train_proc = preproc.transform(X_train)\n",
    "X_val_proc = preproc.transform(X_val)\n",
    "X_test_proc = preproc.transform(X_test)\n",
    "X_train_proc.shape , X_val_proc.shape, X_test_proc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) üîÆ Your predictions in Tensorflow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ This is your first **regression** task with Keras! \n",
    "\n",
    "üí° Here a few tips to get started:\n",
    "- Kaggle's [rule](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) requires to minimize **`rmsle`** (Root Mean Square Log Error). \n",
    "    - As you can see, we can specify `msle` directly as a loss-function with Tensorflow.Keras!\n",
    "    - Just remember to take the square-root of your loss results to read your rmsle metric.\n",
    "    \n",
    "    \n",
    "üòÉ The best boosted-tree ***rmsle*** score to beat is around ***0.13***\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://i.pinimg.com/564x/4c/fe/ef/4cfeef34af09973211f584e8307b433c.jpg\" alt=\"`Impossible mission\" style=\"height: 300px; width:500px;\"/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "‚ùì **Your mission, should you choose to accept it:** ‚ùì\n",
    "- üí™ Beat the best boosted-tree üí™ \n",
    "\n",
    "    - Your responsibilities are:\n",
    "        - to build the ***best neural network architecture*** possible,\n",
    "        - and to control the number of epochs to ***avoid overfitting***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Predicting the houses' prices using a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Preliminary Question: Initializing a Neural Network** ‚ùì\n",
    "\n",
    "Create a function `initialize_model` which initializes a Dense Neural network:\n",
    "- You are responsible for designing the architecture (number of layers, number of neurons)\n",
    "- The function should also compile the model with the following parameters:\n",
    "    - ***optimizer = \"adam\"***\n",
    "    - ***loss = \"msle\"*** (_Optimizing directly for the Squared Log Error!_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": [
     "challengify"
    ],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def initialize_model():\n",
    "    \n",
    "    #############################\n",
    "    #  1 - Model architecture   #\n",
    "    ############################# \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(80, activation='relu' ,input_dim=159)) \n",
    "    model.add(layers.Dense(40, activation='relu'))\n",
    "    model.add(layers.Dense(20, activation='relu')) \n",
    "    model.add(layers.Dense(5, activation='relu')) \n",
    "    \n",
    "    model.add(layers.Dense(1, activation= \"linear\"))\n",
    "    \n",
    "    #############################\n",
    "    #  2 - Optimization Method  #\n",
    "    #############################\n",
    "    model.compile(loss='msle',\n",
    "                  optimizer='adam', ) \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions/Guidance** ‚ùì\n",
    "\n",
    "1. Initialize a Neural Network\n",
    "2. Train it\n",
    "3. Evaluate its performance\n",
    "4. Is the model overfitting the dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": [
     "challengify"
    ],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 123.5253 - val_loss: 103.2947\n",
      "Epoch 2/500\n",
      "32/32 [==============================] - 0s 914us/step - loss: 89.3444 - val_loss: 75.9633\n",
      "Epoch 3/500\n",
      "32/32 [==============================] - 0s 918us/step - loss: 67.5138 - val_loss: 59.3447\n",
      "Epoch 4/500\n",
      "32/32 [==============================] - 0s 888us/step - loss: 54.0132 - val_loss: 48.6528\n",
      "Epoch 5/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 44.2003 - val_loss: 38.4284\n",
      "Epoch 6/500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 34.6165 - val_loss: 30.8914\n",
      "Epoch 7/500\n",
      "32/32 [==============================] - 0s 881us/step - loss: 28.4735 - val_loss: 25.8547\n",
      "Epoch 8/500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 24.0473 - val_loss: 21.9941\n",
      "Epoch 9/500\n",
      "32/32 [==============================] - 0s 864us/step - loss: 20.5595 - val_loss: 18.9074\n",
      "Epoch 10/500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 17.7790 - val_loss: 16.4394\n",
      "Epoch 11/500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 15.5403 - val_loss: 14.4375\n",
      "Epoch 12/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 13.7049 - val_loss: 12.7819\n",
      "Epoch 13/500\n",
      "32/32 [==============================] - 0s 867us/step - loss: 12.1738 - val_loss: 11.3823\n",
      "Epoch 14/500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 10.8710 - val_loss: 10.1869\n",
      "Epoch 15/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 9.7499 - val_loss: 9.1510\n",
      "Epoch 16/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 8.7745 - val_loss: 8.2439\n",
      "Epoch 17/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 7.9159 - val_loss: 7.4415\n",
      "Epoch 18/500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 7.1524 - val_loss: 6.7240\n",
      "Epoch 19/500\n",
      "32/32 [==============================] - 0s 866us/step - loss: 6.4677 - val_loss: 6.0813\n",
      "Epoch 20/500\n",
      "32/32 [==============================] - 0s 875us/step - loss: 5.8523 - val_loss: 5.5022\n",
      "Epoch 21/500\n",
      "32/32 [==============================] - 0s 822us/step - loss: 5.2978 - val_loss: 4.9796\n",
      "Epoch 22/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 4.7977 - val_loss: 4.5090\n",
      "Epoch 23/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 4.3473 - val_loss: 4.0843\n",
      "Epoch 24/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 3.9407 - val_loss: 3.7038\n",
      "Epoch 25/500\n",
      "32/32 [==============================] - 0s 881us/step - loss: 3.5756 - val_loss: 3.3591\n",
      "Epoch 26/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 3.2456 - val_loss: 3.0498\n",
      "Epoch 27/500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 2.9486 - val_loss: 2.7705\n",
      "Epoch 28/500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 2.6804 - val_loss: 2.5185\n",
      "Epoch 29/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 2.4385 - val_loss: 2.2902\n",
      "Epoch 30/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 2.2196 - val_loss: 2.0842\n",
      "Epoch 31/500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 2.0213 - val_loss: 1.8985\n",
      "Epoch 32/500\n",
      "32/32 [==============================] - 0s 873us/step - loss: 1.8416 - val_loss: 1.7290\n",
      "Epoch 33/500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 1.6756 - val_loss: 1.5688\n",
      "Epoch 34/500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 1.5182 - val_loss: 1.4177\n",
      "Epoch 35/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 1.3705 - val_loss: 1.2772\n",
      "Epoch 36/500\n",
      "32/32 [==============================] - 0s 815us/step - loss: 1.2338 - val_loss: 1.1492\n",
      "Epoch 37/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 1.1097 - val_loss: 1.0328\n",
      "Epoch 38/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.9975 - val_loss: 0.9281\n",
      "Epoch 39/500\n",
      "32/32 [==============================] - 0s 831us/step - loss: 0.8967 - val_loss: 0.8344\n",
      "Epoch 40/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.8064 - val_loss: 0.7506\n",
      "Epoch 41/500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.7257 - val_loss: 0.6758\n",
      "Epoch 42/500\n",
      "32/32 [==============================] - 0s 817us/step - loss: 0.6537 - val_loss: 0.6090\n",
      "Epoch 43/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.5877 - val_loss: 0.5457\n",
      "Epoch 44/500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.5237 - val_loss: 0.4840\n",
      "Epoch 45/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.4629 - val_loss: 0.4269\n",
      "Epoch 46/500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.4075 - val_loss: 0.3765\n",
      "Epoch 47/500\n",
      "32/32 [==============================] - 0s 831us/step - loss: 0.3591 - val_loss: 0.3323\n",
      "Epoch 48/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3168 - val_loss: 0.2947\n",
      "Epoch 49/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.2808 - val_loss: 0.2621\n",
      "Epoch 50/500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.2499 - val_loss: 0.2348\n",
      "Epoch 51/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.2237 - val_loss: 0.2118\n",
      "Epoch 52/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2017 - val_loss: 0.1924\n",
      "Epoch 53/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.1830 - val_loss: 0.1763\n",
      "Epoch 54/500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.1674 - val_loss: 0.1627\n",
      "Epoch 55/500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.1543 - val_loss: 0.1514\n",
      "Epoch 56/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.1435 - val_loss: 0.1419\n",
      "Epoch 57/500\n",
      "32/32 [==============================] - 0s 815us/step - loss: 0.1343 - val_loss: 0.1343\n",
      "Epoch 58/500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.1268 - val_loss: 0.1280\n",
      "Epoch 59/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.1206 - val_loss: 0.1226\n",
      "Epoch 60/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.1154 - val_loss: 0.1183\n",
      "Epoch 61/500\n",
      "32/32 [==============================] - 0s 812us/step - loss: 0.1111 - val_loss: 0.1148\n",
      "Epoch 62/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.1075 - val_loss: 0.1118\n",
      "Epoch 63/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.1045 - val_loss: 0.1094\n",
      "Epoch 64/500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.1020 - val_loss: 0.1073\n",
      "Epoch 65/500\n",
      "32/32 [==============================] - 0s 880us/step - loss: 0.0999 - val_loss: 0.1058\n",
      "Epoch 66/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0983 - val_loss: 0.1045\n",
      "Epoch 67/500\n",
      "32/32 [==============================] - 0s 864us/step - loss: 0.0969 - val_loss: 0.1035\n",
      "Epoch 68/500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.0958 - val_loss: 0.1027\n",
      "Epoch 69/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0949 - val_loss: 0.1021\n",
      "Epoch 70/500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.0942 - val_loss: 0.1015\n",
      "Epoch 71/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0936 - val_loss: 0.1010\n",
      "Epoch 72/500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.0931 - val_loss: 0.1007\n",
      "Epoch 73/500\n",
      "32/32 [==============================] - 0s 864us/step - loss: 0.0926 - val_loss: 0.1003\n",
      "Epoch 74/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0922 - val_loss: 0.1000\n",
      "Epoch 75/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0919 - val_loss: 0.0997\n",
      "Epoch 76/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0915 - val_loss: 0.0994\n",
      "Epoch 77/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.0912 - val_loss: 0.0991\n",
      "Epoch 78/500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.0909 - val_loss: 0.0988\n",
      "Epoch 79/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0906 - val_loss: 0.0985\n",
      "Epoch 80/500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.0903 - val_loss: 0.0982\n",
      "Epoch 81/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0900 - val_loss: 0.0979\n",
      "Epoch 82/500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.0897 - val_loss: 0.0976\n",
      "Epoch 83/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0894 - val_loss: 0.0973\n",
      "Epoch 84/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0891 - val_loss: 0.0970\n",
      "Epoch 85/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0888 - val_loss: 0.0967\n",
      "Epoch 86/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0885 - val_loss: 0.0964\n",
      "Epoch 87/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0882 - val_loss: 0.0960\n",
      "Epoch 88/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0879 - val_loss: 0.0957\n",
      "Epoch 89/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0876 - val_loss: 0.0954\n",
      "Epoch 90/500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.0873 - val_loss: 0.0950\n",
      "Epoch 91/500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.0869 - val_loss: 0.0947\n",
      "Epoch 92/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0866 - val_loss: 0.0943\n",
      "Epoch 93/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0863 - val_loss: 0.0939\n",
      "Epoch 94/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0859 - val_loss: 0.0936\n",
      "Epoch 95/500\n",
      "32/32 [==============================] - 0s 826us/step - loss: 0.0856 - val_loss: 0.0932\n",
      "Epoch 96/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0853 - val_loss: 0.0928\n",
      "Epoch 97/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.0849 - val_loss: 0.0924\n",
      "Epoch 98/500\n",
      "32/32 [==============================] - 0s 831us/step - loss: 0.0846 - val_loss: 0.0921\n",
      "Epoch 99/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0842 - val_loss: 0.0917\n",
      "Epoch 100/500\n",
      "32/32 [==============================] - 0s 828us/step - loss: 0.0839 - val_loss: 0.0913\n",
      "Epoch 101/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0835 - val_loss: 0.0909\n",
      "Epoch 102/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0832 - val_loss: 0.0905\n",
      "Epoch 103/500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.0828 - val_loss: 0.0901\n",
      "Epoch 104/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0824 - val_loss: 0.0897\n",
      "Epoch 105/500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.0820 - val_loss: 0.0892\n",
      "Epoch 106/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.0817 - val_loss: 0.0888\n",
      "Epoch 107/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0813 - val_loss: 0.0884\n",
      "Epoch 108/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0809 - val_loss: 0.0880\n",
      "Epoch 109/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0805 - val_loss: 0.0875\n",
      "Epoch 110/500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.0801 - val_loss: 0.0871\n",
      "Epoch 111/500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0797 - val_loss: 0.0866\n",
      "Epoch 112/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0793 - val_loss: 0.0862\n",
      "Epoch 113/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0789 - val_loss: 0.0858\n",
      "Epoch 114/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0785 - val_loss: 0.0853\n",
      "Epoch 115/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0781 - val_loss: 0.0848\n",
      "Epoch 116/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0777 - val_loss: 0.0844\n",
      "Epoch 117/500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.0772 - val_loss: 0.0839\n",
      "Epoch 118/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0768 - val_loss: 0.0834\n",
      "Epoch 119/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0764 - val_loss: 0.0829\n",
      "Epoch 120/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0759 - val_loss: 0.0824\n",
      "Epoch 121/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0755 - val_loss: 0.0819\n",
      "Epoch 122/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0750 - val_loss: 0.0815\n",
      "Epoch 123/500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.0746 - val_loss: 0.0810\n",
      "Epoch 124/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0742 - val_loss: 0.0805\n",
      "Epoch 125/500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.0737 - val_loss: 0.0800\n",
      "Epoch 126/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0733 - val_loss: 0.0795\n",
      "Epoch 127/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0728 - val_loss: 0.0789\n",
      "Epoch 128/500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.0723 - val_loss: 0.0784\n",
      "Epoch 129/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0719 - val_loss: 0.0779\n",
      "Epoch 130/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0714 - val_loss: 0.0774\n",
      "Epoch 131/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0710 - val_loss: 0.0769\n",
      "Epoch 132/500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.0705 - val_loss: 0.0763\n",
      "Epoch 133/500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.0700 - val_loss: 0.0758\n",
      "Epoch 134/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0696 - val_loss: 0.0753\n",
      "Epoch 135/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0691 - val_loss: 0.0748\n",
      "Epoch 136/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.0686 - val_loss: 0.0743\n",
      "Epoch 137/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0681 - val_loss: 0.0737\n",
      "Epoch 138/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0677 - val_loss: 0.0732\n",
      "Epoch 139/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0672 - val_loss: 0.0726\n",
      "Epoch 140/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0667 - val_loss: 0.0721\n",
      "Epoch 141/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0662 - val_loss: 0.0715\n",
      "Epoch 142/500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.0657 - val_loss: 0.0710\n",
      "Epoch 143/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0652 - val_loss: 0.0704\n",
      "Epoch 144/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0647 - val_loss: 0.0699\n",
      "Epoch 145/500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.0642 - val_loss: 0.0693\n",
      "Epoch 146/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0638 - val_loss: 0.0688\n",
      "Epoch 147/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0633 - val_loss: 0.0682\n",
      "Epoch 148/500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.0628 - val_loss: 0.0677\n",
      "Epoch 149/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0623 - val_loss: 0.0671\n",
      "Epoch 150/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0618 - val_loss: 0.0665\n",
      "Epoch 151/500\n",
      "32/32 [==============================] - 0s 817us/step - loss: 0.0613 - val_loss: 0.0660\n",
      "Epoch 152/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0608 - val_loss: 0.0654\n",
      "Epoch 153/500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.0603 - val_loss: 0.0649\n",
      "Epoch 154/500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.0598 - val_loss: 0.0643\n",
      "Epoch 155/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0593 - val_loss: 0.0637\n",
      "Epoch 156/500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.0588 - val_loss: 0.0631\n",
      "Epoch 157/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0583 - val_loss: 0.0626\n",
      "Epoch 158/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0578 - val_loss: 0.0620\n",
      "Epoch 159/500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.0573 - val_loss: 0.0615\n",
      "Epoch 160/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0568 - val_loss: 0.0609\n",
      "Epoch 161/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0563 - val_loss: 0.0603\n",
      "Epoch 162/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0558 - val_loss: 0.0598\n",
      "Epoch 163/500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.0553 - val_loss: 0.0592\n",
      "Epoch 164/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0548 - val_loss: 0.0586\n",
      "Epoch 165/500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.0544 - val_loss: 0.0582\n",
      "Epoch 166/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0539 - val_loss: 0.0576\n",
      "Epoch 167/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0534 - val_loss: 0.0570\n",
      "Epoch 168/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.0529 - val_loss: 0.0565\n",
      "Epoch 169/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0524 - val_loss: 0.0559\n",
      "Epoch 170/500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.0519 - val_loss: 0.0554\n",
      "Epoch 171/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0515 - val_loss: 0.0549\n",
      "Epoch 172/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0510 - val_loss: 0.0543\n",
      "Epoch 173/500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.0505 - val_loss: 0.0538\n",
      "Epoch 174/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0501 - val_loss: 0.0532\n",
      "Epoch 175/500\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.0496 - val_loss: 0.0527\n",
      "Epoch 176/500\n",
      "32/32 [==============================] - 0s 812us/step - loss: 0.0492 - val_loss: 0.0522\n",
      "Epoch 177/500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.0487 - val_loss: 0.0517\n",
      "Epoch 178/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.0483 - val_loss: 0.0512\n",
      "Epoch 179/500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.0478 - val_loss: 0.0507\n",
      "Epoch 180/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0474 - val_loss: 0.0502\n",
      "Epoch 181/500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.0470 - val_loss: 0.0497\n",
      "Epoch 182/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0465 - val_loss: 0.0492\n",
      "Epoch 183/500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.0462 - val_loss: 0.0488\n",
      "Epoch 184/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0457 - val_loss: 0.0482\n",
      "Epoch 185/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0453 - val_loss: 0.0478\n",
      "Epoch 186/500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.0449 - val_loss: 0.0473\n",
      "Epoch 187/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0445 - val_loss: 0.0469\n",
      "Epoch 188/500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.0441 - val_loss: 0.0464\n",
      "Epoch 189/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0437 - val_loss: 0.0460\n",
      "Epoch 190/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0433 - val_loss: 0.0455\n",
      "Epoch 191/500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.0429 - val_loss: 0.0451\n",
      "Epoch 192/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0425 - val_loss: 0.0447\n",
      "Epoch 193/500\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.0421 - val_loss: 0.0443\n",
      "Epoch 194/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0418 - val_loss: 0.0438\n",
      "Epoch 195/500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.0414 - val_loss: 0.0435\n",
      "Epoch 196/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0411 - val_loss: 0.0431\n",
      "Epoch 197/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0427\n",
      "Epoch 198/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0403 - val_loss: 0.0423\n",
      "Epoch 199/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0400 - val_loss: 0.0419\n",
      "Epoch 200/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0397 - val_loss: 0.0416\n",
      "Epoch 201/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0394 - val_loss: 0.0412\n",
      "Epoch 202/500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.0390 - val_loss: 0.0409\n",
      "Epoch 203/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0387 - val_loss: 0.0405\n",
      "Epoch 204/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0384 - val_loss: 0.0402\n",
      "Epoch 205/500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.0381 - val_loss: 0.0399\n",
      "Epoch 206/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0378 - val_loss: 0.0395\n",
      "Epoch 207/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0375 - val_loss: 0.0392\n",
      "Epoch 208/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0373 - val_loss: 0.0389\n",
      "Epoch 209/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0369 - val_loss: 0.0386\n",
      "Epoch 210/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0366 - val_loss: 0.0383\n",
      "Epoch 211/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0364 - val_loss: 0.0380\n",
      "Epoch 212/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0361 - val_loss: 0.0377\n",
      "Epoch 213/500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.0358 - val_loss: 0.0374\n",
      "Epoch 214/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0356 - val_loss: 0.0372\n",
      "Epoch 215/500\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.0353 - val_loss: 0.0369\n",
      "Epoch 216/500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0350 - val_loss: 0.0366\n",
      "Epoch 217/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0348 - val_loss: 0.0364\n",
      "Epoch 218/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0345 - val_loss: 0.0361\n",
      "Epoch 219/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0343 - val_loss: 0.0359\n",
      "Epoch 220/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0341 - val_loss: 0.0356\n",
      "Epoch 221/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0338 - val_loss: 0.0354\n",
      "Epoch 222/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.0336 - val_loss: 0.0352\n",
      "Epoch 223/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0334 - val_loss: 0.0350\n",
      "Epoch 224/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.0331 - val_loss: 0.0347\n",
      "Epoch 225/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0329 - val_loss: 0.0345\n",
      "Epoch 226/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0327 - val_loss: 0.0343\n",
      "Epoch 227/500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0325 - val_loss: 0.0341\n",
      "Epoch 228/500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.0323 - val_loss: 0.0339\n",
      "Epoch 229/500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.0321 - val_loss: 0.0337\n",
      "Epoch 230/500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.0319 - val_loss: 0.0335\n",
      "Epoch 231/500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.0317 - val_loss: 0.0334\n",
      "Epoch 232/500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0315 - val_loss: 0.0332\n",
      "Epoch 233/500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0313 - val_loss: 0.0330\n",
      "Epoch 234/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0311 - val_loss: 0.0328\n",
      "Epoch 235/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0309 - val_loss: 0.0327\n",
      "Epoch 236/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.0307 - val_loss: 0.0325\n",
      "Epoch 237/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0306 - val_loss: 0.0324\n",
      "Epoch 238/500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.0304 - val_loss: 0.0322\n",
      "Epoch 239/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.0303 - val_loss: 0.0320\n",
      "Epoch 240/500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.0301 - val_loss: 0.0319\n",
      "Epoch 241/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0299 - val_loss: 0.0317\n",
      "Epoch 242/500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.0297 - val_loss: 0.0316\n",
      "Epoch 243/500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.0295 - val_loss: 0.0315\n",
      "Epoch 244/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0294 - val_loss: 0.0313\n",
      "Epoch 245/500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.0292 - val_loss: 0.0312\n",
      "Epoch 246/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0310\n",
      "Epoch 247/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0290 - val_loss: 0.0309\n",
      "Epoch 248/500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.0288 - val_loss: 0.0308\n",
      "Epoch 249/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0286 - val_loss: 0.0306\n",
      "Epoch 250/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.0284 - val_loss: 0.0305\n",
      "Epoch 251/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0284 - val_loss: 0.0304\n",
      "Epoch 252/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0303\n",
      "Epoch 253/500\n",
      "32/32 [==============================] - 0s 884us/step - loss: 0.0280 - val_loss: 0.0302\n",
      "Epoch 254/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0279 - val_loss: 0.0300\n",
      "Epoch 255/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0277 - val_loss: 0.0299\n",
      "Epoch 256/500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.0276 - val_loss: 0.0298\n",
      "Epoch 257/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0274 - val_loss: 0.0297\n",
      "Epoch 258/500\n",
      "32/32 [==============================] - 0s 812us/step - loss: 0.0272 - val_loss: 0.0296\n",
      "Epoch 259/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0271 - val_loss: 0.0294\n",
      "Epoch 260/500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.0270 - val_loss: 0.0293\n",
      "Epoch 261/500\n",
      "32/32 [==============================] - 0s 867us/step - loss: 0.0268 - val_loss: 0.0292\n",
      "Epoch 262/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0267 - val_loss: 0.0291\n",
      "Epoch 263/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0265 - val_loss: 0.0290\n",
      "Epoch 264/500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.0264 - val_loss: 0.0289\n",
      "Epoch 265/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0263 - val_loss: 0.0288\n",
      "Epoch 266/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.0261 - val_loss: 0.0287\n",
      "Epoch 267/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0260 - val_loss: 0.0286\n",
      "Epoch 268/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0258 - val_loss: 0.0284\n",
      "Epoch 269/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0257 - val_loss: 0.0284\n",
      "Epoch 270/500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.0256 - val_loss: 0.0282\n",
      "Epoch 271/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0254 - val_loss: 0.0281\n",
      "Epoch 272/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0253 - val_loss: 0.0280\n",
      "Epoch 273/500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.0252 - val_loss: 0.0280\n",
      "Epoch 274/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0251 - val_loss: 0.0278\n",
      "Epoch 275/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0249 - val_loss: 0.0278\n",
      "Epoch 276/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0248 - val_loss: 0.0276\n",
      "Epoch 277/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0247 - val_loss: 0.0276\n",
      "Epoch 278/500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.0245 - val_loss: 0.0275\n",
      "Epoch 279/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0244 - val_loss: 0.0274\n",
      "Epoch 280/500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.0243 - val_loss: 0.0273\n",
      "Epoch 281/500\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.0241 - val_loss: 0.0272\n",
      "Epoch 282/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0271\n",
      "Epoch 283/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0240 - val_loss: 0.0270\n",
      "Epoch 284/500\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.0238 - val_loss: 0.0269\n",
      "Epoch 285/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0237 - val_loss: 0.0268\n",
      "Epoch 286/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0236 - val_loss: 0.0267\n",
      "Epoch 287/500\n",
      "32/32 [==============================] - 0s 935us/step - loss: 0.0234 - val_loss: 0.0266\n",
      "Epoch 288/500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0234 - val_loss: 0.0265\n",
      "Epoch 289/500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.0233 - val_loss: 0.0264\n",
      "Epoch 290/500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.0231 - val_loss: 0.0264\n",
      "Epoch 291/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.0230 - val_loss: 0.0263\n",
      "Epoch 292/500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.0229 - val_loss: 0.0262\n",
      "Epoch 293/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0228 - val_loss: 0.0261\n",
      "Epoch 294/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.0227 - val_loss: 0.0260\n",
      "Epoch 295/500\n",
      "32/32 [==============================] - 0s 812us/step - loss: 0.0226 - val_loss: 0.0259\n",
      "Epoch 296/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0225 - val_loss: 0.0258\n",
      "Epoch 297/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0224 - val_loss: 0.0257\n",
      "Epoch 298/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0223 - val_loss: 0.0257\n",
      "Epoch 299/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0222 - val_loss: 0.0256\n",
      "Epoch 300/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0220 - val_loss: 0.0255\n",
      "Epoch 301/500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.0220 - val_loss: 0.0254\n",
      "Epoch 302/500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.0219 - val_loss: 0.0253\n",
      "Epoch 303/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0218 - val_loss: 0.0252\n",
      "Epoch 304/500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.0217 - val_loss: 0.0252\n",
      "Epoch 305/500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.0216 - val_loss: 0.0250\n",
      "Epoch 306/500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.0215 - val_loss: 0.0249\n",
      "Epoch 307/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0213 - val_loss: 0.0249\n",
      "Epoch 308/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.0248\n",
      "Epoch 309/500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.0212 - val_loss: 0.0247\n",
      "Epoch 310/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0211 - val_loss: 0.0246\n",
      "Epoch 311/500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.0210 - val_loss: 0.0245\n",
      "Epoch 312/500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.0209 - val_loss: 0.0244\n",
      "Epoch 313/500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.0208 - val_loss: 0.0244\n",
      "Epoch 314/500\n",
      "32/32 [==============================] - 0s 829us/step - loss: 0.0207 - val_loss: 0.0243\n",
      "Epoch 315/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0206 - val_loss: 0.0242\n",
      "Epoch 316/500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0206 - val_loss: 0.0241\n",
      "Epoch 317/500\n",
      "32/32 [==============================] - 0s 831us/step - loss: 0.0205 - val_loss: 0.0241\n",
      "Epoch 318/500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.0204 - val_loss: 0.0240\n",
      "Epoch 319/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0203 - val_loss: 0.0240\n",
      "Epoch 320/500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.0203 - val_loss: 0.0239\n",
      "Epoch 321/500\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.0202 - val_loss: 0.0238\n",
      "Epoch 322/500\n",
      "32/32 [==============================] - 0s 867us/step - loss: 0.0201 - val_loss: 0.0238\n",
      "Epoch 323/500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.0200 - val_loss: 0.0237\n",
      "Epoch 324/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0199 - val_loss: 0.0237\n",
      "Epoch 325/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0198 - val_loss: 0.0236\n",
      "Epoch 326/500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.0198 - val_loss: 0.0235\n",
      "Epoch 327/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0197 - val_loss: 0.0235\n",
      "Epoch 328/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0196 - val_loss: 0.0234\n",
      "Epoch 329/500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.0196 - val_loss: 0.0233\n",
      "Epoch 330/500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.0195 - val_loss: 0.0233\n",
      "Epoch 331/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0194 - val_loss: 0.0232\n",
      "Epoch 332/500\n",
      "32/32 [==============================] - 0s 815us/step - loss: 0.0193 - val_loss: 0.0232\n",
      "Epoch 333/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0193 - val_loss: 0.0231\n",
      "Epoch 334/500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0192 - val_loss: 0.0231\n",
      "Epoch 335/500\n",
      "32/32 [==============================] - 0s 831us/step - loss: 0.0192 - val_loss: 0.0231\n",
      "Epoch 336/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0230\n",
      "Epoch 337/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0229\n",
      "Epoch 338/500\n",
      "32/32 [==============================] - 0s 873us/step - loss: 0.0190 - val_loss: 0.0229\n",
      "Epoch 339/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0189 - val_loss: 0.0228\n",
      "Epoch 340/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0189 - val_loss: 0.0227\n",
      "Epoch 341/500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.0188 - val_loss: 0.0227\n",
      "Epoch 342/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.0187 - val_loss: 0.0226\n",
      "Epoch 343/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.0186 - val_loss: 0.0226\n",
      "Epoch 344/500\n",
      "32/32 [==============================] - 0s 828us/step - loss: 0.0186 - val_loss: 0.0226\n",
      "Epoch 345/500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.0186 - val_loss: 0.0225\n",
      "Epoch 346/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0184 - val_loss: 0.0225\n",
      "Epoch 347/500\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.0185 - val_loss: 0.0224\n",
      "Epoch 348/500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.0184 - val_loss: 0.0224\n",
      "Epoch 349/500\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.0183 - val_loss: 0.0223\n",
      "Epoch 350/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0182 - val_loss: 0.0223\n",
      "Epoch 351/500\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.0182 - val_loss: 0.0222\n",
      "Epoch 352/500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.0181 - val_loss: 0.0222\n",
      "Epoch 353/500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.0180 - val_loss: 0.0222\n",
      "Epoch 354/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0180 - val_loss: 0.0221\n",
      "Epoch 355/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0180 - val_loss: 0.0221\n",
      "Epoch 356/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.0179 - val_loss: 0.0220\n",
      "Epoch 357/500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.0178 - val_loss: 0.0220\n",
      "Epoch 358/500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.0177 - val_loss: 0.0220\n",
      "Epoch 359/500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.0178 - val_loss: 0.0219\n",
      "Epoch 360/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0176 - val_loss: 0.0218\n",
      "Epoch 361/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0218\n",
      "Epoch 362/500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.0176 - val_loss: 0.0218\n",
      "Epoch 363/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0175 - val_loss: 0.0217\n",
      "Epoch 364/500\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.0174 - val_loss: 0.0216\n",
      "Epoch 365/500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0174 - val_loss: 0.0217\n",
      "Epoch 366/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.0174 - val_loss: 0.0216\n",
      "Epoch 367/500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0172 - val_loss: 0.0216\n",
      "Epoch 368/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0172 - val_loss: 0.0215\n",
      "Epoch 369/500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0171 - val_loss: 0.0215\n",
      "Epoch 370/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0171 - val_loss: 0.0214\n",
      "Epoch 371/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0171 - val_loss: 0.0214\n",
      "Epoch 372/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0170 - val_loss: 0.0214\n",
      "Epoch 373/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0169 - val_loss: 0.0213\n",
      "Epoch 374/500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.0169 - val_loss: 0.0213\n",
      "Epoch 375/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.0168 - val_loss: 0.0213\n",
      "Epoch 376/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0169 - val_loss: 0.0213\n",
      "Epoch 377/500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.0167 - val_loss: 0.0211\n",
      "Epoch 378/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0167 - val_loss: 0.0211\n",
      "Epoch 379/500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.0166 - val_loss: 0.0212\n",
      "Epoch 380/500\n",
      "32/32 [==============================] - 0s 885us/step - loss: 0.0166 - val_loss: 0.0210\n",
      "Epoch 381/500\n",
      "32/32 [==============================] - 0s 873us/step - loss: 0.0165 - val_loss: 0.0210\n",
      "Epoch 382/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0210\n",
      "Epoch 383/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0210\n",
      "Epoch 384/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0209\n",
      "Epoch 385/500\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.0162 - val_loss: 0.0208\n",
      "Epoch 386/500\n",
      "32/32 [==============================] - 0s 909us/step - loss: 0.0162 - val_loss: 0.0209\n",
      "Epoch 387/500\n",
      "32/32 [==============================] - 0s 921us/step - loss: 0.0163 - val_loss: 0.0208\n",
      "Epoch 388/500\n",
      "32/32 [==============================] - 0s 987us/step - loss: 0.0162 - val_loss: 0.0208\n",
      "Epoch 389/500\n",
      "32/32 [==============================] - 0s 962us/step - loss: 0.0161 - val_loss: 0.0207\n",
      "Epoch 390/500\n",
      "32/32 [==============================] - 0s 929us/step - loss: 0.0160 - val_loss: 0.0207\n",
      "Epoch 391/500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.0160 - val_loss: 0.0206\n",
      "Epoch 392/500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.0159 - val_loss: 0.0206\n",
      "Epoch 393/500\n",
      "32/32 [==============================] - 0s 902us/step - loss: 0.0158 - val_loss: 0.0206\n",
      "Epoch 394/500\n",
      "32/32 [==============================] - 0s 934us/step - loss: 0.0158 - val_loss: 0.0206\n",
      "Epoch 395/500\n",
      "32/32 [==============================] - 0s 877us/step - loss: 0.0157 - val_loss: 0.0205\n",
      "Epoch 396/500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.0157 - val_loss: 0.0207\n",
      "Epoch 397/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0159 - val_loss: 0.0204\n",
      "Epoch 398/500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.0157 - val_loss: 0.0204\n",
      "Epoch 399/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0156 - val_loss: 0.0204\n",
      "Epoch 400/500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.0155 - val_loss: 0.0204\n",
      "Epoch 401/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0155 - val_loss: 0.0203\n",
      "Epoch 402/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0154 - val_loss: 0.0203\n",
      "Epoch 403/500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.0154 - val_loss: 0.0203\n",
      "Epoch 404/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0202\n",
      "Epoch 405/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0202\n",
      "Epoch 406/500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.0152 - val_loss: 0.0202\n",
      "Epoch 407/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0152 - val_loss: 0.0201\n",
      "Epoch 408/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0151 - val_loss: 0.0201\n",
      "Epoch 409/500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.0151 - val_loss: 0.0201\n",
      "Epoch 410/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0151 - val_loss: 0.0200\n",
      "Epoch 411/500\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.0151 - val_loss: 0.0200\n",
      "Epoch 412/500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.0149 - val_loss: 0.0200\n",
      "Epoch 413/500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0149 - val_loss: 0.0200\n",
      "Epoch 414/500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.0149 - val_loss: 0.0200\n",
      "Epoch 415/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0148 - val_loss: 0.0199\n",
      "Epoch 416/500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.0148 - val_loss: 0.0199\n",
      "Epoch 417/500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.0148 - val_loss: 0.0200\n",
      "Epoch 418/500\n",
      "32/32 [==============================] - 0s 867us/step - loss: 0.0147 - val_loss: 0.0199\n",
      "Epoch 419/500\n",
      "32/32 [==============================] - 0s 883us/step - loss: 0.0147 - val_loss: 0.0198\n",
      "Epoch 420/500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.0146 - val_loss: 0.0199\n",
      "Epoch 421/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0146 - val_loss: 0.0198\n",
      "Epoch 422/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0197\n",
      "Epoch 423/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0197\n",
      "Epoch 424/500\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0144 - val_loss: 0.0197\n",
      "Epoch 425/500\n",
      "32/32 [==============================] - 0s 899us/step - loss: 0.0144 - val_loss: 0.0197\n",
      "Epoch 426/500\n",
      "32/32 [==============================] - 0s 908us/step - loss: 0.0143 - val_loss: 0.0196\n",
      "Epoch 427/500\n",
      "32/32 [==============================] - 0s 947us/step - loss: 0.0143 - val_loss: 0.0196\n",
      "Epoch 428/500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.0143 - val_loss: 0.0196\n",
      "Epoch 429/500\n",
      "32/32 [==============================] - 0s 829us/step - loss: 0.0143 - val_loss: 0.0195\n",
      "Epoch 430/500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.0142 - val_loss: 0.0199\n",
      "Epoch 431/500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.0142 - val_loss: 0.0195\n",
      "Epoch 432/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0142 - val_loss: 0.0195\n",
      "Epoch 433/500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.0141 - val_loss: 0.0194\n",
      "Epoch 434/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0140 - val_loss: 0.0195\n",
      "Epoch 435/500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.0140 - val_loss: 0.0194\n",
      "Epoch 436/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.0140 - val_loss: 0.0193\n",
      "Epoch 437/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0139 - val_loss: 0.0193\n",
      "Epoch 438/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0139 - val_loss: 0.0193\n",
      "Epoch 439/500\n",
      "32/32 [==============================] - 0s 826us/step - loss: 0.0138 - val_loss: 0.0193\n",
      "Epoch 440/500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.0138 - val_loss: 0.0192\n",
      "Epoch 441/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.0192\n",
      "Epoch 442/500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.0137 - val_loss: 0.0191\n",
      "Epoch 443/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0136 - val_loss: 0.0192\n",
      "Epoch 444/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0136 - val_loss: 0.0196\n",
      "Epoch 445/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0137 - val_loss: 0.0192\n",
      "Epoch 446/500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.0135 - val_loss: 0.0190\n",
      "Epoch 447/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0135 - val_loss: 0.0191\n",
      "Epoch 448/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0136 - val_loss: 0.0190\n",
      "Epoch 449/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0135 - val_loss: 0.0190\n",
      "Epoch 450/500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.0136 - val_loss: 0.0191\n",
      "Epoch 451/500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.0134 - val_loss: 0.0190\n",
      "Epoch 452/500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.0133 - val_loss: 0.0190\n",
      "Epoch 453/500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.0133 - val_loss: 0.0190\n",
      "Epoch 454/500\n",
      "32/32 [==============================] - 0s 866us/step - loss: 0.0134 - val_loss: 0.0189\n",
      "Epoch 455/500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.0132 - val_loss: 0.0192\n",
      "Epoch 456/500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.0131 - val_loss: 0.0189\n",
      "Epoch 457/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0132 - val_loss: 0.0188\n",
      "Epoch 458/500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.0131 - val_loss: 0.0188\n",
      "Epoch 459/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0130 - val_loss: 0.0189\n",
      "Epoch 460/500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.0130 - val_loss: 0.0188\n",
      "Epoch 461/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0188\n",
      "Epoch 462/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0188\n",
      "Epoch 463/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0128 - val_loss: 0.0187\n",
      "Epoch 464/500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.0128 - val_loss: 0.0186\n",
      "Epoch 465/500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.0128 - val_loss: 0.0186\n",
      "Epoch 466/500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.0127 - val_loss: 0.0186\n",
      "Epoch 467/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0127 - val_loss: 0.0186\n",
      "Epoch 468/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0127 - val_loss: 0.0186\n",
      "Epoch 469/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0126 - val_loss: 0.0185\n",
      "Epoch 470/500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.0126 - val_loss: 0.0185\n",
      "Epoch 471/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0125 - val_loss: 0.0185\n",
      "Epoch 472/500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.0125 - val_loss: 0.0185\n",
      "Epoch 473/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.0124 - val_loss: 0.0185\n",
      "Epoch 474/500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.0124 - val_loss: 0.0184\n",
      "Epoch 475/500\n",
      "32/32 [==============================] - 0s 829us/step - loss: 0.0124 - val_loss: 0.0184\n",
      "Epoch 476/500\n",
      "32/32 [==============================] - 0s 881us/step - loss: 0.0123 - val_loss: 0.0185\n",
      "Epoch 477/500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.0123 - val_loss: 0.0187\n",
      "Epoch 478/500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.0123 - val_loss: 0.0183\n",
      "Epoch 479/500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0123 - val_loss: 0.0183\n",
      "Epoch 480/500\n",
      "32/32 [==============================] - 0s 866us/step - loss: 0.0123 - val_loss: 0.0184\n",
      "Epoch 481/500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.0121 - val_loss: 0.0183\n",
      "Epoch 482/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0185\n",
      "Epoch 483/500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0185\n",
      "Epoch 484/500\n",
      "32/32 [==============================] - 0s 945us/step - loss: 0.0121 - val_loss: 0.0183\n",
      "Epoch 485/500\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.0119 - val_loss: 0.0182\n",
      "Epoch 486/500\n",
      "32/32 [==============================] - 0s 970us/step - loss: 0.0119 - val_loss: 0.0182\n",
      "Epoch 487/500\n",
      "32/32 [==============================] - 0s 959us/step - loss: 0.0121 - val_loss: 0.0185\n",
      "Epoch 488/500\n",
      "32/32 [==============================] - 0s 908us/step - loss: 0.0120 - val_loss: 0.0182\n",
      "Epoch 489/500\n",
      "32/32 [==============================] - 0s 888us/step - loss: 0.0119 - val_loss: 0.0180\n",
      "Epoch 490/500\n",
      "32/32 [==============================] - 0s 917us/step - loss: 0.0118 - val_loss: 0.0181\n",
      "Epoch 491/500\n",
      "32/32 [==============================] - 0s 916us/step - loss: 0.0118 - val_loss: 0.0180\n",
      "Epoch 492/500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0117 - val_loss: 0.0181\n",
      "Epoch 493/500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.0116 - val_loss: 0.0181\n",
      "Epoch 494/500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.0116 - val_loss: 0.0181\n",
      "Epoch 495/500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.0116 - val_loss: 0.0180\n",
      "Epoch 496/500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.0115 - val_loss: 0.0179\n",
      "Epoch 497/500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.0115 - val_loss: 0.0179\n",
      "Epoch 498/500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.0116 - val_loss: 0.0179\n",
      "Epoch 499/500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.0114 - val_loss: 0.0181\n",
      "Epoch 500/500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.0180\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "history = model.fit(X_train_proc, y_train, validation_data = (X_val_proc, y_val), batch_size=32, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ We coded a `plot_history` function that you can use to detect overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(np.sqrt(history.history['loss']))\n",
    "    plt.plot(np.sqrt(history.history['val_loss']))\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('RMSLE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": [
     "challengify"
    ],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 382us/step - loss: 0.0180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13433362301593307"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msle = model.evaluate(X_val_proc, y_val)\n",
    "RMSLE = np.sqrt(msle)\n",
    "RMSLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Challenging yourself"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Questions to challenge yourself:**\n",
    "- Are you satisfied with your score?\n",
    "- Before publishing it, ask yourself whether you could really trust it or not?\n",
    "- Have you cross-validated your neural network? \n",
    "    - Feel free to cross-validate it manually with a *for loop* in Python to make sure that your results are robust against the randomness of a _train-val split_ before before submitting to Kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create a function `evaluate_model` following the framework below üëá then use a for loop with `KFold` to manually cross validate your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def initialize_model(X_train_proc):\n",
    "    \n",
    "    #############################\n",
    "    #  1 - Model architecture   #\n",
    "    ############################# \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(80, activation='relu' ,input_dim=X_train_proc.shape[-1])) \n",
    "    model.add(layers.Dense(40, activation='relu'))\n",
    "    model.add(layers.Dense(20, activation='relu')) \n",
    "    model.add(layers.Dense(5, activation='relu')) \n",
    "    \n",
    "    model.add(layers.Dense(1, activation= \"linear\"))\n",
    "    \n",
    "    #############################\n",
    "    #  2 - Optimization Method  #\n",
    "    #############################\n",
    "    model.compile(loss='msle',\n",
    "                  optimizer='adam', ) \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(X, y, train_index, val_index):\n",
    "    \n",
    "    # Slicing the training set and the validation set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Preprocessing \n",
    "    from utils.preprocessor import create_preproc\n",
    "\n",
    "    preproc = create_preproc(X_train)\n",
    "    preproc.fit(X_train, y_train)\n",
    "    X_train_proc = preproc.transform(X_train)\n",
    "    X_val_proc = preproc.transform(X_val)\n",
    "    X_test_proc = preproc.transform(X_test)\n",
    "    \n",
    "    # Training the model on the preprocessed training dataset\n",
    "    model = initialize_model(X_train_proc)\n",
    "    history = model.fit(X_train_proc, y_train, validation_data = (X_val_proc, y_val), batch_size=32, epochs=100)\n",
    "    \n",
    "    # Evaluating the model on the preprocessed validation dataset\n",
    "    msle = model.evaluate(X_val_proc, y_val)\n",
    "    rmsle = np.sqrt(msle)\n",
    "    \n",
    "    rmsle_min = min(np.sqrt(history.history[\"val_loss\"]))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "                'rmsle_final_epoch': [rmsle],\n",
    "                'rmsle_min': [rmsle_min]\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "cv = 5\n",
    "KF = KFold(n_splits = cv, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 102.7220 - val_loss: 79.7509\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 876us/step - loss: 67.0943 - val_loss: 56.9714\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 927us/step - loss: 49.9705 - val_loss: 44.1735\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 917us/step - loss: 39.5298 - val_loss: 35.6505\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 866us/step - loss: 32.1690 - val_loss: 29.2643\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 26.4771 - val_loss: 24.1863\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 21.9209 - val_loss: 20.1411\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 843us/step - loss: 18.2923 - val_loss: 16.8539\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 851us/step - loss: 15.3042 - val_loss: 14.1347\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 947us/step - loss: 12.8457 - val_loss: 11.9144\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 850us/step - loss: 10.8535 - val_loss: 10.1238\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 841us/step - loss: 9.2473 - val_loss: 8.6715\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 825us/step - loss: 7.9404 - val_loss: 7.4852\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 829us/step - loss: 6.8648 - val_loss: 6.4991\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 833us/step - loss: 5.9679 - val_loss: 5.6700\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 968us/step - loss: 5.2118 - val_loss: 4.9684\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 795us/step - loss: 4.5686 - val_loss: 4.3686\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 783us/step - loss: 4.0178 - val_loss: 3.8515\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 839us/step - loss: 3.5421 - val_loss: 3.4055\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 784us/step - loss: 3.1304 - val_loss: 3.0169\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 830us/step - loss: 2.7719 - val_loss: 2.6761\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 806us/step - loss: 2.4576 - val_loss: 2.3783\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 891us/step - loss: 2.1824 - val_loss: 2.1160\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.9404 - val_loss: 1.8853\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 815us/step - loss: 1.7272 - val_loss: 1.6818\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 820us/step - loss: 1.5389 - val_loss: 1.5018\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 812us/step - loss: 1.3727 - val_loss: 1.3414\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 810us/step - loss: 1.2255 - val_loss: 1.1995\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 790us/step - loss: 1.0950 - val_loss: 1.0738\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.9794 - val_loss: 0.9615\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.8765 - val_loss: 0.8622\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.7852 - val_loss: 0.7736\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.7041 - val_loss: 0.6938\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.6315 - val_loss: 0.6233\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.5669 - val_loss: 0.5602\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 956us/step - loss: 0.5094 - val_loss: 0.5039\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 984us/step - loss: 0.4585 - val_loss: 0.4532\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.4129 - val_loss: 0.4084\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.3729 - val_loss: 0.3691\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.3373 - val_loss: 0.3340\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.3060 - val_loss: 0.3032\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.2786 - val_loss: 0.2751\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.2542 - val_loss: 0.2512\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.2330 - val_loss: 0.2297\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.2144 - val_loss: 0.2110\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.1981 - val_loss: 0.1945\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 923us/step - loss: 0.1839 - val_loss: 0.1802\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.1717 - val_loss: 0.1676\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 805us/step - loss: 0.1611 - val_loss: 0.1567\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.1519 - val_loss: 0.1472\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.1440 - val_loss: 0.1389\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.1372 - val_loss: 0.1318\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.1314 - val_loss: 0.1256\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.1264 - val_loss: 0.1202\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.1222 - val_loss: 0.1154\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.1185 - val_loss: 0.1115\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.1154 - val_loss: 0.1081\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1128 - val_loss: 0.1050\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1105 - val_loss: 0.1025\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.1087 - val_loss: 0.1003\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.1070 - val_loss: 0.0983\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.1057 - val_loss: 0.0967\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.1045 - val_loss: 0.0952\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.1035 - val_loss: 0.0939\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.1026 - val_loss: 0.0929\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.1019 - val_loss: 0.0918\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.1012 - val_loss: 0.0910\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.1007 - val_loss: 0.0902\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.1001 - val_loss: 0.0895\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.0997 - val_loss: 0.0890\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.0993 - val_loss: 0.0883\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.0988 - val_loss: 0.0878\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.0985 - val_loss: 0.0873\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.0981 - val_loss: 0.0868\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.0977 - val_loss: 0.0864\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.0974 - val_loss: 0.0860\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.0971 - val_loss: 0.0856\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.0968 - val_loss: 0.0852\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.0964 - val_loss: 0.0848\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.0961 - val_loss: 0.0844\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.0958 - val_loss: 0.0840\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.0954 - val_loss: 0.0837\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.0951 - val_loss: 0.0833\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.0948 - val_loss: 0.0830\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.0944 - val_loss: 0.0826\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0941 - val_loss: 0.0822\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.0938 - val_loss: 0.0819\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.0934 - val_loss: 0.0815\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0930 - val_loss: 0.0811\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0927 - val_loss: 0.0808\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.0923 - val_loss: 0.0804\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.0920 - val_loss: 0.0800\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.0916 - val_loss: 0.0797\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.0912 - val_loss: 0.0793\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.0908 - val_loss: 0.0790\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.0904 - val_loss: 0.0785\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.0900 - val_loss: 0.0781\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.0896 - val_loss: 0.0777\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.0892 - val_loss: 0.0773\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.0888 - val_loss: 0.0769\n",
      "10/10 [==============================] - 0s 400us/step - loss: 0.0769\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 116.6476 - val_loss: 92.9941\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 825us/step - loss: 78.6328 - val_loss: 67.2358\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 792us/step - loss: 59.9415 - val_loss: 53.5667\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 805us/step - loss: 48.7322 - val_loss: 44.3158\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 836us/step - loss: 40.6017 - val_loss: 37.2974\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 859us/step - loss: 34.4981 - val_loss: 32.0618\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 864us/step - loss: 29.8881 - val_loss: 28.0211\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 857us/step - loss: 26.2711 - val_loss: 24.7931\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 23.3409 - val_loss: 22.1362\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 20.9049 - val_loss: 19.9071\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 850us/step - loss: 18.8442 - val_loss: 18.0000\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 835us/step - loss: 17.0725 - val_loss: 16.3523\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 795us/step - loss: 15.5322 - val_loss: 14.9129\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 792us/step - loss: 14.1804 - val_loss: 13.6405\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 821us/step - loss: 12.9011 - val_loss: 12.3128\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 11.5847 - val_loss: 11.0209\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 10.3429 - val_loss: 9.8253\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 886us/step - loss: 9.1991 - val_loss: 8.7331\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 827us/step - loss: 8.1681 - val_loss: 7.7617\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 827us/step - loss: 7.2599 - val_loss: 6.9114\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 801us/step - loss: 6.4677 - val_loss: 6.1727\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 830us/step - loss: 5.7797 - val_loss: 5.5281\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 847us/step - loss: 5.1791 - val_loss: 4.9662\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 824us/step - loss: 4.6533 - val_loss: 4.4708\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 822us/step - loss: 4.1879 - val_loss: 4.0308\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 781us/step - loss: 3.7744 - val_loss: 3.6377\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 848us/step - loss: 3.4045 - val_loss: 3.2873\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 797us/step - loss: 3.0735 - val_loss: 2.9715\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 813us/step - loss: 2.7755 - val_loss: 2.6880\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 800us/step - loss: 2.5071 - val_loss: 2.4316\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 817us/step - loss: 2.2651 - val_loss: 2.2011\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 804us/step - loss: 2.0471 - val_loss: 1.9924\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 810us/step - loss: 1.8506 - val_loss: 1.8040\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 817us/step - loss: 1.6735 - val_loss: 1.6342\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 826us/step - loss: 1.5134 - val_loss: 1.4811\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 1.3693 - val_loss: 1.3429\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 833us/step - loss: 1.2391 - val_loss: 1.2183\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 1.1218 - val_loss: 1.1058\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 805us/step - loss: 1.0159 - val_loss: 1.0042\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.9203 - val_loss: 0.9122\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.8342 - val_loss: 0.8291\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.7566 - val_loss: 0.7545\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 996us/step - loss: 0.6866 - val_loss: 0.6872\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.6234 - val_loss: 0.6268\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 952us/step - loss: 0.5667 - val_loss: 0.5715\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.5155 - val_loss: 0.5221\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.4692 - val_loss: 0.4777\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.4279 - val_loss: 0.4371\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.3904 - val_loss: 0.4013\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.3570 - val_loss: 0.3688\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.3269 - val_loss: 0.3397\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.3000 - val_loss: 0.3134\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.2758 - val_loss: 0.2901\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.2541 - val_loss: 0.2692\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.2349 - val_loss: 0.2502\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.2175 - val_loss: 0.2334\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.2021 - val_loss: 0.2184\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.1884 - val_loss: 0.2048\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.1762 - val_loss: 0.1927\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1653 - val_loss: 0.1823\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.1558 - val_loss: 0.1727\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.1473 - val_loss: 0.1642\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.1398 - val_loss: 0.1567\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.1331 - val_loss: 0.1503\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.1273 - val_loss: 0.1444\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.1221 - val_loss: 0.1392\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.1176 - val_loss: 0.1346\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1136 - val_loss: 0.1307\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.1101 - val_loss: 0.1271\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1071 - val_loss: 0.1239\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.1044 - val_loss: 0.1211\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.1021 - val_loss: 0.1187\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.1001 - val_loss: 0.1166\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0983 - val_loss: 0.1148\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.0967 - val_loss: 0.1131\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.0954 - val_loss: 0.1116\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0942 - val_loss: 0.1103\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 791us/step - loss: 0.0931 - val_loss: 0.1091\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0922 - val_loss: 0.1081\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.0914 - val_loss: 0.1072\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.0906 - val_loss: 0.1064\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.0900 - val_loss: 0.1056\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.0893 - val_loss: 0.1049\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 778us/step - loss: 0.0888 - val_loss: 0.1043\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.0883 - val_loss: 0.1037\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.0879 - val_loss: 0.1031\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.0874 - val_loss: 0.1026\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.0870 - val_loss: 0.1021\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.0866 - val_loss: 0.1017\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.0862 - val_loss: 0.1012\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.0859 - val_loss: 0.1008\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.0856 - val_loss: 0.1004\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.0852 - val_loss: 0.1000\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.0849 - val_loss: 0.0996\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.0846 - val_loss: 0.0992\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.0843 - val_loss: 0.0989\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.0839 - val_loss: 0.0985\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.0836 - val_loss: 0.0981\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.0833 - val_loss: 0.0977\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.0830 - val_loss: 0.0974\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.0974\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 127.3197 - val_loss: 106.6814\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 858us/step - loss: 89.7147 - val_loss: 75.1787\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 822us/step - loss: 65.8713 - val_loss: 57.8482\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 814us/step - loss: 52.1004 - val_loss: 47.0331\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 851us/step - loss: 42.9876 - val_loss: 39.3178\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 841us/step - loss: 36.1765 - val_loss: 33.3699\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 863us/step - loss: 30.9046 - val_loss: 28.7406\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 858us/step - loss: 26.7713 - val_loss: 25.0725\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 23.4612 - val_loss: 22.0929\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 910us/step - loss: 20.7481 - val_loss: 19.6273\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 807us/step - loss: 18.4831 - val_loss: 17.5503\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 16.5624 - val_loss: 15.7714\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 825us/step - loss: 14.8650 - val_loss: 14.1249\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 815us/step - loss: 13.2952 - val_loss: 12.6434\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 790us/step - loss: 11.9093 - val_loss: 11.3471\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 820us/step - loss: 10.7039 - val_loss: 10.2239\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 9.6552 - val_loss: 9.2444\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 842us/step - loss: 8.7376 - val_loss: 8.3820\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 7.9286 - val_loss: 7.6209\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 860us/step - loss: 7.2118 - val_loss: 6.9415\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 850us/step - loss: 6.5712 - val_loss: 6.3346\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 885us/step - loss: 5.9971 - val_loss: 5.7884\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 854us/step - loss: 5.4798 - val_loss: 5.2947\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 850us/step - loss: 5.0117 - val_loss: 4.8487\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 991us/step - loss: 4.5874 - val_loss: 4.4423\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 4.1987 - val_loss: 4.0640\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 816us/step - loss: 3.8331 - val_loss: 3.7060\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 831us/step - loss: 3.4851 - val_loss: 3.3653\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 847us/step - loss: 3.1570 - val_loss: 3.0465\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 843us/step - loss: 2.8534 - val_loss: 2.7548\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 857us/step - loss: 2.5769 - val_loss: 2.4900\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 841us/step - loss: 2.3264 - val_loss: 2.2490\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 826us/step - loss: 2.0927 - val_loss: 2.0195\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 830us/step - loss: 1.8706 - val_loss: 1.7992\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.6554 - val_loss: 1.5853\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 965us/step - loss: 1.4479 - val_loss: 1.3822\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 865us/step - loss: 1.2544 - val_loss: 1.1941\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 844us/step - loss: 1.0786 - val_loss: 1.0269\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.9235 - val_loss: 0.8802\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.7885 - val_loss: 0.7542\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.6732 - val_loss: 0.6466\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.5753 - val_loss: 0.5551\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.4928 - val_loss: 0.4782\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.4235 - val_loss: 0.4142\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.3659 - val_loss: 0.3599\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 884us/step - loss: 0.3176 - val_loss: 0.3152\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2775 - val_loss: 0.2772\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.2441 - val_loss: 0.2461\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.2165 - val_loss: 0.2205\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1939 - val_loss: 0.1988\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1751 - val_loss: 0.1813\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.1599 - val_loss: 0.1666\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.1474 - val_loss: 0.1551\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.1374 - val_loss: 0.1458\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.1295 - val_loss: 0.1380\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.1229 - val_loss: 0.1315\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.1176 - val_loss: 0.1266\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.1136 - val_loss: 0.1224\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.1103 - val_loss: 0.1192\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.1076 - val_loss: 0.1166\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1055 - val_loss: 0.1145\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.1039 - val_loss: 0.1127\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.1025 - val_loss: 0.1113\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.1014 - val_loss: 0.1102\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.1006 - val_loss: 0.1092\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.0998 - val_loss: 0.1083\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.0992 - val_loss: 0.1077\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.0987 - val_loss: 0.1071\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.0982 - val_loss: 0.1065\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.0978 - val_loss: 0.1060\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.0974 - val_loss: 0.1055\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.0970 - val_loss: 0.1051\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0966 - val_loss: 0.1047\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.0963 - val_loss: 0.1043\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.0959 - val_loss: 0.1039\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.0956 - val_loss: 0.1035\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.0952 - val_loss: 0.1031\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.0949 - val_loss: 0.1027\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.0945 - val_loss: 0.1024\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.0942 - val_loss: 0.1020\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.0938 - val_loss: 0.1016\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.0934 - val_loss: 0.1012\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.0931 - val_loss: 0.1008\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0927 - val_loss: 0.1004\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 959us/step - loss: 0.0923 - val_loss: 0.1000\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.0919 - val_loss: 0.0996\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.0915 - val_loss: 0.0992\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.0912 - val_loss: 0.0987\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.0908 - val_loss: 0.0984\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.0903 - val_loss: 0.0979\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.0899 - val_loss: 0.0975\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.0895 - val_loss: 0.0970\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0891 - val_loss: 0.0966\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0886 - val_loss: 0.0961\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 0.0957\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 999us/step - loss: 0.0878 - val_loss: 0.0952\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 943us/step - loss: 0.0873 - val_loss: 0.0947\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.0868 - val_loss: 0.0942\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.0864 - val_loss: 0.0938\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.0859 - val_loss: 0.0933\n",
      "10/10 [==============================] - 0s 441us/step - loss: 0.0933\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 118.1097 - val_loss: 89.6233\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 854us/step - loss: 73.8609 - val_loss: 59.3494\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 824us/step - loss: 51.8809 - val_loss: 44.1283\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 39.9195 - val_loss: 34.9566\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 846us/step - loss: 32.0734 - val_loss: 28.3159\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 852us/step - loss: 26.2257 - val_loss: 23.3627\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 850us/step - loss: 21.8757 - val_loss: 19.6585\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 897us/step - loss: 18.5790 - val_loss: 16.7999\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 841us/step - loss: 15.9692 - val_loss: 14.4702\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 816us/step - loss: 13.8135 - val_loss: 12.5371\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 838us/step - loss: 12.0246 - val_loss: 10.9345\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 835us/step - loss: 10.5353 - val_loss: 9.5966\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 9.2854 - val_loss: 8.4672\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 973us/step - loss: 8.2247 - val_loss: 7.5024\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 7.3097 - val_loss: 6.6623\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 922us/step - loss: 6.5063 - val_loss: 5.9209\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 816us/step - loss: 5.7947 - val_loss: 5.2631\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 830us/step - loss: 5.1637 - val_loss: 4.6799\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 808us/step - loss: 4.6034 - val_loss: 4.1649\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 818us/step - loss: 4.1077 - val_loss: 3.7097\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 857us/step - loss: 3.6689 - val_loss: 3.3071\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 811us/step - loss: 3.2807 - val_loss: 2.9502\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 857us/step - loss: 2.9364 - val_loss: 2.6349\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 848us/step - loss: 2.6309 - val_loss: 2.3547\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 829us/step - loss: 2.3595 - val_loss: 2.1069\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 822us/step - loss: 2.1185 - val_loss: 1.8856\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 812us/step - loss: 1.9033 - val_loss: 1.6898\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 786us/step - loss: 1.7118 - val_loss: 1.5138\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 826us/step - loss: 1.5406 - val_loss: 1.3576\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 843us/step - loss: 1.3873 - val_loss: 1.2186\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 842us/step - loss: 1.2505 - val_loss: 1.0939\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 827us/step - loss: 1.1279 - val_loss: 0.9825\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.0180 - val_loss: 0.8833\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.9195 - val_loss: 0.7943\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.8311 - val_loss: 0.7145\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.7520 - val_loss: 0.6434\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.6808 - val_loss: 0.5796\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.6171 - val_loss: 0.5222\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.5598 - val_loss: 0.4715\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.5085 - val_loss: 0.4264\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 877us/step - loss: 0.4627 - val_loss: 0.3856\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.4214 - val_loss: 0.3495\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.3846 - val_loss: 0.3173\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.3515 - val_loss: 0.2886\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.3219 - val_loss: 0.2630\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.2956 - val_loss: 0.2401\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2719 - val_loss: 0.2201\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2510 - val_loss: 0.2023\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.2325 - val_loss: 0.1863\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.2157 - val_loss: 0.1728\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.2010 - val_loss: 0.1608\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1880 - val_loss: 0.1499\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1763 - val_loss: 0.1405\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1660 - val_loss: 0.1325\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 972us/step - loss: 0.1570 - val_loss: 0.1253\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.1490 - val_loss: 0.1190\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.1419 - val_loss: 0.1135\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.1357 - val_loss: 0.1089\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.1303 - val_loss: 0.1050\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.1255 - val_loss: 0.1014\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.1213 - val_loss: 0.0986\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.1177 - val_loss: 0.0960\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.1144 - val_loss: 0.0938\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.1116 - val_loss: 0.0920\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.1091 - val_loss: 0.0905\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.1070 - val_loss: 0.0892\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.1051 - val_loss: 0.0881\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.1035 - val_loss: 0.0872\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.1021 - val_loss: 0.0864\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 890us/step - loss: 0.1008 - val_loss: 0.0858\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.0998 - val_loss: 0.0852\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 970us/step - loss: 0.0988 - val_loss: 0.0848\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 978us/step - loss: 0.0980 - val_loss: 0.0845\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0973 - val_loss: 0.0842\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0966 - val_loss: 0.0839\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.0960 - val_loss: 0.0837\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.0955 - val_loss: 0.0835\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.0950 - val_loss: 0.0833\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.0946 - val_loss: 0.0831\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.0942 - val_loss: 0.0830\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.0938 - val_loss: 0.0829\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.0934 - val_loss: 0.0827\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.0931 - val_loss: 0.0825\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.0928 - val_loss: 0.0824\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 902us/step - loss: 0.0924 - val_loss: 0.0823\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.0921 - val_loss: 0.0821\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 877us/step - loss: 0.0918 - val_loss: 0.0819\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.0915 - val_loss: 0.0818\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0913 - val_loss: 0.0816\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 979us/step - loss: 0.0910 - val_loss: 0.0814\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.0907 - val_loss: 0.0812\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.0904 - val_loss: 0.0810\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0901 - val_loss: 0.0808\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 973us/step - loss: 0.0898 - val_loss: 0.0805\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.0895 - val_loss: 0.0803\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.0892 - val_loss: 0.0801\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.0889 - val_loss: 0.0798\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.0886 - val_loss: 0.0796\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.0883 - val_loss: 0.0793\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.0880 - val_loss: 0.0790\n",
      "10/10 [==============================] - 0s 384us/step - loss: 0.0790\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 122.0358 - val_loss: 97.5444\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 825us/step - loss: 82.3157 - val_loss: 69.3418\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 827us/step - loss: 61.0414 - val_loss: 53.5619\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 849us/step - loss: 48.2377 - val_loss: 43.3638\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 39.6406 - val_loss: 36.1298\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 835us/step - loss: 33.2681 - val_loss: 30.5913\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 846us/step - loss: 28.3776 - val_loss: 26.3155\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 831us/step - loss: 24.5642 - val_loss: 22.9335\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 853us/step - loss: 21.5103 - val_loss: 20.1930\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 791us/step - loss: 19.0097 - val_loss: 17.9221\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 798us/step - loss: 16.9217 - val_loss: 16.0071\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 828us/step - loss: 15.1476 - val_loss: 14.3678\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 841us/step - loss: 13.6150 - val_loss: 12.9351\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 12.2699 - val_loss: 11.6745\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 851us/step - loss: 11.0811 - val_loss: 10.5537\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 817us/step - loss: 10.0235 - val_loss: 9.5572\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 824us/step - loss: 9.0828 - val_loss: 8.6682\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 793us/step - loss: 8.2406 - val_loss: 7.8641\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 808us/step - loss: 7.4588 - val_loss: 7.0982\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 808us/step - loss: 6.7181 - val_loss: 6.3864\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 6.0402 - val_loss: 5.7419\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 985us/step - loss: 5.4323 - val_loss: 5.1700\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 814us/step - loss: 4.8940 - val_loss: 4.6629\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 819us/step - loss: 4.4169 - val_loss: 4.2144\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 779us/step - loss: 3.9944 - val_loss: 3.8155\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 827us/step - loss: 3.6183 - val_loss: 3.4608\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 840us/step - loss: 3.2831 - val_loss: 3.1427\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 838us/step - loss: 2.9830 - val_loss: 2.8589\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 834us/step - loss: 2.7141 - val_loss: 2.6028\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 813us/step - loss: 2.4720 - val_loss: 2.3725\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 806us/step - loss: 2.2534 - val_loss: 2.1648\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0563 - val_loss: 1.9765\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 825us/step - loss: 1.8776 - val_loss: 1.8058\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 821us/step - loss: 1.7157 - val_loss: 1.6509\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 825us/step - loss: 1.5686 - val_loss: 1.5106\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 791us/step - loss: 1.4352 - val_loss: 1.3825\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.3137 - val_loss: 1.2659\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 937us/step - loss: 1.2028 - val_loss: 1.1604\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 844us/step - loss: 1.1021 - val_loss: 1.0634\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 1.0101 - val_loss: 0.9750\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.9261 - val_loss: 0.8949\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.8497 - val_loss: 0.8209\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.7799 - val_loss: 0.7535\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.7160 - val_loss: 0.6926\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.6580 - val_loss: 0.6363\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.6049 - val_loss: 0.5856\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.5567 - val_loss: 0.5390\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.5125 - val_loss: 0.4965\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4722 - val_loss: 0.4578\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 973us/step - loss: 0.4356 - val_loss: 0.4222\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.4021 - val_loss: 0.3902\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 958us/step - loss: 0.3717 - val_loss: 0.3610\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3440 - val_loss: 0.3343\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.3188 - val_loss: 0.3099\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2959 - val_loss: 0.2877\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 897us/step - loss: 0.2751 - val_loss: 0.2674\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.2562 - val_loss: 0.2494\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.2392 - val_loss: 0.2330\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.2237 - val_loss: 0.2179\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.2097 - val_loss: 0.2044\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.1971 - val_loss: 0.1921\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.1857 - val_loss: 0.1812\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.1755 - val_loss: 0.1712\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.1662 - val_loss: 0.1622\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.1579 - val_loss: 0.1541\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.1504 - val_loss: 0.1469\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1437 - val_loss: 0.1405\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.1378 - val_loss: 0.1346\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.1324 - val_loss: 0.1294\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.1276 - val_loss: 0.1248\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.1233 - val_loss: 0.1206\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.1196 - val_loss: 0.1169\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.1162 - val_loss: 0.1136\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.1131 - val_loss: 0.1107\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.1105 - val_loss: 0.1080\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.1081 - val_loss: 0.1057\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.1060 - val_loss: 0.1036\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.1041 - val_loss: 0.1018\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.1025 - val_loss: 0.1002\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 933us/step - loss: 0.1010 - val_loss: 0.0988\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0998 - val_loss: 0.0975\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.0986 - val_loss: 0.0963\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0976 - val_loss: 0.0953\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0967 - val_loss: 0.0944\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.0959 - val_loss: 0.0936\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.0951 - val_loss: 0.0928\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.0945 - val_loss: 0.0922\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0939 - val_loss: 0.0916\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.0933 - val_loss: 0.0910\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.0928 - val_loss: 0.0906\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.0924 - val_loss: 0.0901\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.0920 - val_loss: 0.0897\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.0916 - val_loss: 0.0893\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0912 - val_loss: 0.0889\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 986us/step - loss: 0.0909 - val_loss: 0.0886\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.0905 - val_loss: 0.0883\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.0902 - val_loss: 0.0880\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.0899 - val_loss: 0.0877\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0896 - val_loss: 0.0874\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 943us/step - loss: 0.0893 - val_loss: 0.0871\n",
      "10/10 [==============================] - 0s 377us/step - loss: 0.0871\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for train_index, val_index in KF.split(X):\n",
    "    results.append(evaluate_model(X,y, train_index, val_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   rmsle_final_epoch  rmsle_min\n",
       " 0           0.277354   0.277354,\n",
       "    rmsle_final_epoch  rmsle_min\n",
       " 0           0.312067   0.312067,\n",
       "    rmsle_final_epoch  rmsle_min\n",
       " 0           0.305402   0.305402,\n",
       "    rmsle_final_epoch  rmsle_min\n",
       " 0           0.281057   0.281057,\n",
       "    rmsle_final_epoch  rmsle_min\n",
       " 0           0.295182   0.295182]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) (Bonus) Using all your CPU cores to run Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî• **BONUS** üî• **Multiprocessing computing using [dask](https://docs.dask.org/en/latest/delayed.html)** and **all your CPU cores**:\n",
    "\n",
    "_(to mimic SkLearn's `n_jobs=-1`)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --quiet dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from dask import delayed\n",
    "\n",
    "# cv = 5\n",
    "# kf = KFold(n_splits = cv, shuffle = True)\n",
    "# f = delayed(evaluate_model)\n",
    "\n",
    "# results = delayed([f(X, y, train_index, val_index) for (train_index, val_index) in kf.split(X)\n",
    "#                   ]).compute(\n",
    "#                       scheduler='processes', num_workers=8)\n",
    "\n",
    "# pd.concat(results, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (2.4) (Bonus) Multiprocessing with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "**multiprocessing with default Python library**\n",
    "\n",
    "References :\n",
    "* [Yitong Ren - Speeding Up and Perfecting Your Work Using Parallel Computing](https://towardsdatascience.com/speeding-up-and-perfecting-your-work-using-parallel-computing-8bc2f0c073f8)\n",
    "* [Johaupt Github - Parallel Processing for Cross Validation - BROKEN LINK](https://johaupt.github.io/python/parallel%20processing/cross-validation/multiprocessing_cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# This code will fail try to debug it yourself if you cannot checkout the hints below\n",
    "import multiprocessing as mp\n",
    "pool = mp.Pool(processes=2) #mp.cpu_count()-1)\n",
    "\n",
    "results = []\n",
    "def log_result(x):\n",
    "    results.append(x)\n",
    "    \n",
    "for train_index, val_index in kf.split(X):\n",
    "    pool.apply_async(\n",
    "        evaluate_model,\n",
    "        args=(X, y, train_index, val_index),\n",
    "        callback = log_result)\n",
    "\n",
    "# Close the pool for new tasks\n",
    "pool.close()\n",
    "\n",
    "# Wait for all tasks to complete at this point\n",
    "pool.join()\n",
    "\n",
    "result = pd.concat(results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary markdown='span'>Hints</summary>\n",
    "\n",
    "This is a limitation of multiprocessing in ipython enviroments this code would work fine in .py file.\n",
    "The key error is `AttributeError: Can't get attribute 'evaluate_model' on <module 'main' (built-in)>`\n",
    "\n",
    "Checkout this stackoverflow for a workaround https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror !\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) üèÖFINAL SUBMISSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü¶Ñ Predict the ***prices of the houses in your test set*** and submit your results to Kaggle! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# X_test = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_test_raw.csv\")\n",
    "# X_test_preproc = preproc.transform(X_test)\n",
    "# ALREADY DONE ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíæ Save your predictions in a Dataframe called `results` with the format required by Kaggle so that when you export it to a `.csv`, Kaggle can read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì§  Export your results using Kaggle's submission format and submit it online!\n",
    "\n",
    "_(Uncomment the last cell of this notebook)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# results.to_csv(\"submission_final.csv\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üèÅ Congratulations!\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... it's time for the Recap!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
